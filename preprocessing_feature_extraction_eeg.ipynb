{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Preprocessing Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an environment with Python >= 3.11 to avoid conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/GRUNECO/eeg_harmonization.git\n",
    "!pip install -r eeg_harmonization/requirements.txt\n",
    "!pip install -e eeg_harmonization/.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are loads of dependencies to install so please wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sovaharmony\n",
    "from sovaharmony.preprocessing import harmonize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If desired, install scorEpochs to quantify epoch quality (i.e., select best quality epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'scorepochs_py' already exists and is not an empty directory.\n",
      "['COPYING', '.git', 'README.md', 'scorEpochs_demo.ipynb', 'scorepochs.py']\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Scorepochs-tools/scorepochs_py\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"/home/stud/timlin/bhome/DiffusionEEG/scorepochs_py/\"))\n",
    "import sys\n",
    "sys.path.append(\"/home/stud/timlin/bhome/DiffusionEEG/scorepochs_py/\")\n",
    "\n",
    "from scorepochs import scorEpochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COPYING', '.git', 'README.md', 'scorEpochs_demo.ipynb', '__pycache__', 'scorepochs.py']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"/home/stud/timlin/bhome/DiffusionEEG/scorepochs_py\"))\n",
    "import sys\n",
    "sys.path.append(\"/home/stud/timlin/bhome/DiffusionEEG/scorepochs_py\")\n",
    "\n",
    "from scorepochs import scorEpochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install cmasher\n",
    "%pip install fooof\n",
    "%pip install mne_connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from mne_bids import BIDSPath, read_raw_bids, print_dir_tree, make_report, get_entities_from_fname\n",
    "import scipy.signal as signal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "import sovaharmony\n",
    "from sovaharmony.preprocessing import harmonize\n",
    "\n",
    "import bids\n",
    "from pyprep.prep_pipeline import PrepPipeline\n",
    "from mne.datasets.eegbci import standardize\n",
    "import cmasher as cmr\n",
    "import scipy as sp\n",
    "import fooof\n",
    "from fooof.analysis import get_band_peak_fm\n",
    "from fooof import FOOOF, FOOOFGroup\n",
    "from fooof.bands import Bands\n",
    "from fooof.objs import combine_fooofs\n",
    "from fooof.utils import trim_spectrum\n",
    "from fooof.analysis.error import compute_pointwise_error_fm\n",
    "# import antropy as ant\n",
    "# import yasa\n",
    "# import autogluon\n",
    "from scorepochs import scorEpochs\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, balanced_accuracy_score, accuracy_score, precision_recall_curve\n",
    "\n",
    "import mne_connectivity\n",
    "from mne_connectivity.viz import plot_sensors_connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAU-EEG (Korea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) DOWNLOAD caueeg_bids.rar from Drive\n",
    "\n",
    "\n",
    "2) Just change here the input_path\n",
    "\n",
    "\n",
    "\n",
    "NB: SOVAHARMONY REQUIRES USE UPPERCASE FOR CHANNEL NAMES (if using with other dataset, take this into consideration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "THE_DATASET={\n",
    "    'name':'korea',\n",
    "    'input_path':'data',\n",
    "    'layout':{'extension':'.vhdr','suffix':'eeg', 'task':'eyesClosed','return_type':'filename'},\n",
    "       'args':{'line_freqs':[60]},\n",
    "        'group_regex':None,\n",
    "        'events_to_keep':None,\n",
    "        'run-label': '',\n",
    "        'channels':['FP1', 'F3', 'C3', 'P3', 'O1', 'FP2', 'F4', 'C4', 'P4', 'O2', 'F7', 'T3', 'T5', 'F8', 'T4', 'T6', 'FZ', 'CZ', 'PZ'],        \n",
    "        'spatial_filter':None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "sovaharmony.preprocessing.harmonize(THE_DATASET, fast_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CAUEEG - Korea #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "BIDS root does not exist: /mnt/beegfs/home/timlin/DiffusionEEG/D:/data_analysis/eeg_datasets/bids/korea",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m DATASET\u001b[38;5;241m=\u001b[39mkorea \u001b[38;5;66;03m#DEFINE DATASET\u001b[39;00m\n\u001b[1;32m     19\u001b[0m layoutd \u001b[38;5;241m=\u001b[39m DATASET\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 21\u001b[0m layout \u001b[38;5;241m=\u001b[39m \u001b[43mbids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBIDSLayout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mderivatives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m eegs \u001b[38;5;241m=\u001b[39m layout\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlayoutd)\n\u001b[1;32m     23\u001b[0m eegs \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m eegs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meyesClosed_desc-reject\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m k]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/bids/layout/layout.py:135\u001b[0m, in \u001b[0;36mBIDSLayout.__init__\u001b[0;34m(self, root, validate, absolute_paths, derivatives, config, sources, regex_search, database_path, reset_database, indexer, is_derivative, **indexer_kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m     config \u001b[38;5;241m=\u001b[39m info\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Validate that a valid BIDS project exists at root\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m root, description \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_root\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m([\n\u001b[1;32m    137\u001b[0m     is_derivative,\n\u001b[1;32m    138\u001b[0m     description \u001b[38;5;129;01mand\u001b[39;00m description\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetType\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mderivative\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m ]):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/bids/layout/validation.py:69\u001b[0m, in \u001b[0;36mvalidate_root\u001b[0;34m(root, validate)\u001b[0m\n\u001b[1;32m     67\u001b[0m root \u001b[38;5;241m=\u001b[39m root\u001b[38;5;241m.\u001b[39mabsolute()\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m root\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBIDS root does not exist: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m root)\n\u001b[1;32m     71\u001b[0m target \u001b[38;5;241m=\u001b[39m root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_description.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m target\u001b[38;5;241m.\u001b[39mexists():\n",
      "\u001b[0;31mValueError\u001b[0m: BIDS root does not exist: /mnt/beegfs/home/timlin/DiffusionEEG/D:/data_analysis/eeg_datasets/bids/korea"
     ]
    }
   ],
   "source": [
    "korea={\n",
    "'layout':{'extension':'.fif', 'suffix':'eeg', 'return_type':'filename'},\n",
    "    'ch_names':['FP1', 'F3', 'C3', 'P3', 'O1', 'FP2', 'F4', 'C4', 'P4', 'O2', 'F7', 'T3', 'T5', 'F8', 'T4', 'T6', 'FZ', 'CZ', 'PZ'],\n",
    "    'path':'data'\n",
    "}\n",
    "\n",
    "\n",
    "#Path of the BIDS folder\n",
    "bids_root = 'data'\n",
    "#Seleccionar solo EEG\n",
    "datatype = 'eeg'\n",
    "suffix = 'eeg'\n",
    "\n",
    "#Tarea\n",
    "task = 'eyesClosed' \n",
    "\n",
    "DATASET=korea #DEFINE DATASET\n",
    "\n",
    "layoutd = DATASET.get('layout', None)\n",
    "\n",
    "layout = bids.BIDSLayout(DATASET.get('path', None), derivatives=True)\n",
    "eegs = layout.get(**layoutd)\n",
    "eegs = [k for k in eegs if 'eyesClosed_desc-reject' in k]\n",
    "\n",
    "print(len(eegs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture\n",
    "dict_list = []\n",
    "\n",
    "for eeg_file in eegs: #read and preload dataset\n",
    "    subject_info = layout.parse_file_entities(eeg_file)\n",
    "    session = subject_info.get('session')\n",
    "    subject = subject_info.get('subject')\n",
    "    bids_path = BIDSPath(subject=subject, session=session, task=task, root=bids_root, datatype='eeg')\n",
    "    \n",
    "    epochs = mne.read_epochs(eeg_file, preload = True)\n",
    "    features = {}\n",
    "    features['center'] = 'korea'\n",
    "    features['subject'] = ('k_' + subject)\n",
    "    features['channels'] = epochs.ch_names\n",
    "    features['num_ch'] = len(epochs.ch_names)\n",
    "    features['epoch_num'] = epochs.get_data().shape[0]\n",
    "    features['sfreq'] = epochs.info['sfreq']\n",
    "    dict_list.append(features)\n",
    "description = pd.DataFrame(dict_list)\n",
    "description['min_epochs'] = np.min(description['epoch_num'])\n",
    "\n",
    "description.to_csv('D:/data_analysis/eeg_datasets/bids/korea/derivatives/sovaharmony/inspection.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korea={\n",
    "'layout':{'extension':'.fif', 'suffix':'eeg', 'return_type':'filename'},\n",
    "    'ch_names':['FP1', 'F3', 'C3', 'P3', 'O1', 'FP2', 'F4', 'C4', 'P4', 'O2', 'F7', 'T3', 'T5', 'F8', 'T4', 'T6', 'FZ', 'CZ', 'PZ'],\n",
    "    'path':'D:/data_analysis/eeg_datasets/bids/korea/'\n",
    "}\n",
    "\n",
    "#Path of the BIDS folder\n",
    "bids_root = 'D:/data_analysis/eeg_datasets/bids/korea/'\n",
    "#Seleccionar solo EEG\n",
    "datatype = 'eeg'\n",
    "suffix = 'eeg'\n",
    "\n",
    "#Tarea\n",
    "task = 'eyesClosed' \n",
    "\n",
    "DATASET=korea #DEFINE DATASET\n",
    "\n",
    "layoutd = DATASET.get('layout', None)\n",
    "\n",
    "layout = bids.BIDSLayout(DATASET.get('path', None), derivatives=True)\n",
    "eegs = layout.get(**layoutd)\n",
    "eegs = [k for k in eegs if 'eyesClosed_desc-reject' in k]\n",
    "\n",
    "print(len(eegs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "## Entropy ###\n",
    "\n",
    "dict_list = []\n",
    "for eeg_file in eegs: #read and preload dataset\n",
    "    subject_info = layout.parse_file_entities(eeg_file)\n",
    "    session = subject_info.get('session')\n",
    "    subject = subject_info.get('subject')\n",
    "    bids_path = BIDSPath(subject=subject, session=session, task=task, root=bids_root, datatype='eeg')\n",
    "    epochs = mne.read_epochs(eeg_file, preload = True)\n",
    "    mapping = {'T3': 'T7', 'T4': 'T8', 'T5': 'P7', 'T6': 'P8'}\n",
    "    epochs.rename_channels(mapping)\n",
    "    standardize(epochs) #standardize ch_names\n",
    "    keep = ['Fp1','Fp2', 'F3','F4', 'Fz', 'F7', 'F8',  'C3', 'C4', 'Cz' , 'T7','T8',  'P7' ,'P8', 'P3','P4', 'Pz', 'O1', 'O2']\n",
    "    epochs.pick_channels(keep)\n",
    "    epochs = epochs.filter(l_freq=1, h_freq=45) #bandpassing 1-30Hz\n",
    "    downsample = 128 #downsampling to 128Hz\n",
    "    epochs.resample(downsample)\n",
    "    nepochs,nchannels,npoints = epochs._data.shape\n",
    "    if nepochs >= 20:\n",
    "        clean_epochs = epochs\n",
    "         # get shape of clean epoched data and reshape to fit scorEpochs input requirements\n",
    "        continuous_signal = np.reshape(clean_epochs,(nchannels,nepochs*npoints),order='F') # final shape nchannels, npoints\n",
    "        t_ep = 5 #epoch length\n",
    "        fs = downsample #sampling freq\n",
    "        freq = [1, 30] #interest freq\n",
    "        cfg = {'freqRange':freq, 'fs':fs, 'windowL':t_ep}\n",
    "        idx_best, epoch, scores = scorEpochs(cfg, continuous_signal)\n",
    "        # Get the top 20 epoch indices from idx_best\n",
    "        best_20_indices = idx_best[:20]\n",
    "        # Use these indices to extract the best 20 epochs from clean_epochs\n",
    "        best_20_epochs = epochs[best_20_indices]\n",
    "        # Optionally, you can concatenate the epochs (though it's not strictly necessary)\n",
    "        epochs_c = mne.concatenate_epochs([best_20_epochs], add_offset=False, on_mismatch='raise', verbose=None)\n",
    "        \n",
    "        epochs = epochs_c\n",
    "        epochs.set_montage(\"standard_1005\")\n",
    "        nepochs,nchannels,npoints = epochs._data.shape\n",
    "        correct_channels = keep\n",
    "        epochs.reorder_channels(correct_channels)\n",
    "        channels = epochs.info['ch_names']\n",
    "        srate = downsample\n",
    "        for ch,ch_label in enumerate(channels):\n",
    "            for ep in range(nepochs):\n",
    "                features = {}\n",
    "                features['center'] = 'korea'\n",
    "                features['subject'] = ('kor_' + subject)\n",
    "                features['channel'] = ch_label\n",
    "                features['epoch'] = ep\n",
    "                features['permutation'] = ant.perm_entropy(epochs.get_data()[ep,ch,:], normalize=True)\n",
    "                features['sample'] = ant.sample_entropy(epochs.get_data()[ep,ch,:])\n",
    "                features['approximate'] = ant.app_entropy(epochs.get_data()[ep,ch,:])\n",
    "                features['svd_ent'] = ant.svd_entropy(epochs.get_data()[ep,ch,:], order=5, normalize=True)             \n",
    "                features['higuchi_fd']  = ant.higuchi_fd(epochs.get_data()[ep,ch,:])  \n",
    "                features['hjort_mobility']  = (ant.hjorth_params(epochs.get_data()[ep,ch,:]))[0]\n",
    "                features['hjort_complexity']  = (ant.hjorth_params(epochs.get_data()[ep,ch,:]))[1]\n",
    "                features['detrended_fluct']  = ant.detrended_fluctuation(epochs.get_data()[ep,ch,:])\n",
    "                features['katz_fd']  = ant.katz_fd(epochs.get_data()[ep,ch,:])\n",
    "                features['petrosian_fd']  = ant.petrosian_fd(epochs.get_data()[ep,ch,:])\n",
    "                dict_list.append(features)\n",
    "entropies = pd.DataFrame(dict_list)\n",
    "\n",
    "permutation = pd.DataFrame(entropies.groupby(['center','subject', 'channel']).permutation.mean())\n",
    "entropies.rename(columns = {'sample':'sample_ent'}, inplace = True)\n",
    "sample_ent = pd.DataFrame(entropies.groupby(['center', 'subject', 'channel']).sample_ent.mean())\n",
    "approximate = pd.DataFrame(entropies.groupby(['center', 'subject', 'channel']).approximate.mean())\n",
    "svd_ent = pd.DataFrame(entropies.groupby(['center', 'subject', 'channel']).svd_ent.mean())\n",
    "higuchi_fd = pd.DataFrame(entropies.groupby(['center', 'subject', 'channel']).higuchi_fd.mean())\n",
    "hjort_mobility = pd.DataFrame(entropies.groupby(['center', 'subject', 'channel']).hjort_mobility.mean())\n",
    "hjort_complexity = pd.DataFrame(entropies.groupby(['center', 'subject', 'channel']).hjort_complexity.mean())\n",
    "detrended_fluct = pd.DataFrame(entropies.groupby(['center', 'subject', 'channel']).detrended_fluct.mean())\n",
    "katz_fd = pd.DataFrame(entropies.groupby(['center', 'subject', 'channel']).katz_fd.mean())\n",
    "petrosian_fd = pd.DataFrame(entropies.groupby(['center', 'subject', 'channel']).petrosian_fd.mean())\n",
    "\n",
    "dfs = [permutation, sample_ent, approximate, svd_ent, higuchi_fd, hjort_mobility, hjort_complexity, detrended_fluct, katz_fd, petrosian_fd]\n",
    "entropies_ave = pd.DataFrame(pd.concat(dfs, axis = 1)).reset_index()\n",
    "\n",
    "entropies_ave.to_feather('D:/data_analysis/papers_alberto/cau_combat/features_data/ent_korea.feather')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anterior = [\"Fp1\", \"Fp2\", \"F3\", \"F4\", \"F7\", \"F8\", \"Fz\"]\n",
    "central = [\"T7\", \"T8\", \"C3\", \"C4\", \"Cz\"]\n",
    "posterior = [\"P3\", \"P4\", \"P7\", \"P8\", \"Pz\", \"O1\", \"O2\"]\n",
    "\n",
    "montage = mne.channels.make_standard_montage(\"standard_1005\")\n",
    "epochs.set_montage(montage)\n",
    "\n",
    "groups = [anterior, central, posterior]\n",
    "group_indices = []  # Create a separate list to store indices\n",
    "\n",
    "for ch_names in groups:\n",
    "    indices = [epochs.ch_names.index(ch_name) for ch_name in ch_names]\n",
    "    group_indices.append(indices)  # Append indices to the separate list\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the sensors\n",
    "mne.viz.plot_sensors(epochs.info, ch_groups=group_indices, pointsize=100, show_names=True, linewidth=0, sphere=(0, 0.024, 0, 0.09), axes=ax)\n",
    "plt.show()\n",
    "# Save the figure as TIFF with 300 DPI\n",
    "fig.savefig('D:/data_analysis/papers_alberto/cau_combat/figures/sensors.tiff', dpi=300, format='tiff')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The 10-20 and 10-10 system share 4 channel positions but changed the name. \n",
    "\n",
    "Thus, in the 10-20 the electrodes T3, T4, T5, T6, correspond to the 10-10 T7, T8, P7, and P8 respectively.\n",
    "\n",
    "https://en.wikipedia.org/wiki/10%E2%80%9320_system_(EEG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "dict_list = []\n",
    "for eeg_file in eegs: #read and preload dataset\n",
    "    subject_info = layout.parse_file_entities(eeg_file)\n",
    "    session = subject_info.get('session')\n",
    "    subject = subject_info.get('subject')\n",
    "    bids_path = BIDSPath(subject=subject, session=session, task=task, root=bids_root, datatype='eeg')\n",
    "    epochs = mne.read_epochs(eeg_file, preload = True)\n",
    "    mapping = {'T3': 'T7', 'T4': 'T8', 'T5': 'P7', 'T6': 'P8'}\n",
    "    epochs.rename_channels(mapping)\n",
    "    standardize(epochs) #standardize ch_names\n",
    "    keep = ['Fp1','Fp2', 'F3','F4', 'Fz', 'F7', 'F8',  'C3', 'C4', 'Cz' , 'T7','T8',  'P7' ,'P8', 'P3','P4', 'Pz', 'O1', 'O2']\n",
    "    epochs.pick_channels(keep)\n",
    "    epochs = epochs.filter(l_freq=1, h_freq=45) #bandpassing 1-30Hz\n",
    "    downsample = 128 #downsampling to 128Hz\n",
    "    epochs.resample(downsample)\n",
    "    sf = downsample\n",
    "    nepochs,nchannels,npoints = epochs._data.shape\n",
    "    if nepochs >= 20:\n",
    "        clean_epochs = epochs\n",
    "         # get shape of clean epoched data and reshape to fit scorEpochs input requirements\n",
    "        continuous_signal = np.reshape(clean_epochs,(nchannels,nepochs*npoints),order='F') # final shape nchannels, npoints\n",
    "        t_ep = 5 #epoch length\n",
    "        fs = downsample #sampling freq\n",
    "        freq = [1, 30] #interest freq\n",
    "        cfg = {'freqRange':freq, 'fs':fs, 'windowL':t_ep}\n",
    "        idx_best, epoch, scores = scorEpochs(cfg, continuous_signal)\n",
    "        # Get the top 20 epoch indices from idx_best\n",
    "        best_20_indices = idx_best[:20]\n",
    "        # Use these indices to extract the best 20 epochs from clean_epochs\n",
    "        best_20_epochs = epochs[best_20_indices]\n",
    "        # Optionally, you can concatenate the epochs (though it's not strictly necessary)\n",
    "        epochs_c = mne.concatenate_epochs([best_20_epochs], add_offset=False, on_mismatch='raise', verbose=None)        \n",
    "\n",
    "        epochs = epochs_c\n",
    "        epochs.set_montage(\"standard_1005\")\n",
    "        nepochs,nchannels,npoints = epochs._data.shape\n",
    "        correct_channels = keep\n",
    "        epochs.reorder_channels(correct_channels)\n",
    "        channels = epochs.info['ch_names']\n",
    "        freq_range_slow_theta = (4, 5.5)\n",
    "        freq_range_prealpha = (5.5, 8)\n",
    "        freq_range_alpha = (8, 13)\n",
    "        n_epochs, n_channels, n_points = epochs._data.shape\n",
    "        dominant_freqs = np.zeros(n_epochs)  # Array to store dominant frequencies for each epoch\n",
    "        freq_range_df = (4, 15)\n",
    "\n",
    "        for ch, ch_label in enumerate(channels):\n",
    "            psd, freqs = mne.time_frequency.psd_array_multitaper(epochs.get_data()[:,ch,:], sfreq = sf,fmin=1, fmax=30, n_jobs=-1)\n",
    "            \n",
    "            freq_bins = freqs.shape[0]\n",
    "            band_indices_slow_theta = np.where((freqs >= freq_range_slow_theta[0]) & (freqs < freq_range_slow_theta[1]))[0]\n",
    "            band_indices_prealpha = np.where((freqs >= freq_range_prealpha[0]) & (freqs < freq_range_prealpha[1]))[0]\n",
    "            band_indices_alpha = np.where((freqs >= freq_range_alpha[0]) & (freqs < freq_range_alpha[1]))[0]\n",
    "            band_indices_df = np.where((freqs >= freq_range_df[0]) & (freqs < freq_range_df[1]))[0]\n",
    "\n",
    "            # Compute the dominant frequency for each epoch within the specified range\n",
    "            dominant_freqs = freqs[band_indices_df[np.argmax(psd[:, band_indices_df], axis=1)]]\n",
    "            # Compute the standard deviation of dominant frequencies within the specified range across all epochs\n",
    "            dom_freq = np.mean(dominant_freqs)\n",
    "            dom_freq_var = np.std(dominant_freqs)\n",
    "\n",
    "            freq_prev_slow_theta = np.sum(np.logical_and(freqs[np.argmax(psd, axis=1)] >= freq_range_slow_theta[0],freqs[np.argmax(psd, axis=1)] < freq_range_slow_theta[1])) / n_epochs\n",
    "            freq_prev_prealpha = np.sum(np.logical_and(freqs[np.argmax(psd, axis=1)] >= freq_range_prealpha[0],freqs[np.argmax(psd, axis=1)] < freq_range_prealpha[1])) / n_epochs\n",
    "            freq_prev_alpha = np.sum(np.logical_and(freqs[np.argmax(psd, axis=1)] >= freq_range_alpha[0],freqs[np.argmax(psd, axis=1)] < freq_range_alpha[1])) / n_epochs\n",
    "            \n",
    "            psd = np.median(psd,0) # get median psd vector per channel\n",
    "            unc_bandpowers = yasa.bandpower_from_psd_ndarray(psd, freqs, bands=[(1, 4, 'Delta'), (4, 8, 'Theta'), (8, 13, 'Alpha'), (13, 30, 'Beta')], relative=True)\n",
    "            fm = FOOOF(peak_width_limits=[1, 8], min_peak_height=0.05, max_n_peaks=6)\n",
    "            # Initialize FOOOF object\n",
    "            # Define frequency range across which to model the spectrum\n",
    "            freq_range = [1, 30]\n",
    "            fm.fit(freqs, psd, freq_range)   # Fit the power spectrum model\n",
    "            # Define frequency bands of interest\n",
    "            bands = Bands({'extalpha' : [5, 14],'beta' : [13, 30]})\n",
    "            # Extract params\n",
    "            betas = get_band_peak_fm(fm, bands.beta)\n",
    "            extalphas = get_band_peak_fm(fm, bands.extalpha)\n",
    "\n",
    "            aperiodic = fm.get_params('aperiodic_params')\n",
    "            fit = [fm.get_params('r_squared'), fm.get_params('error')]\n",
    "            features = {}\n",
    "            features['center'] = 'korea'\n",
    "            features['subject'] = ('kor_' + subject)\n",
    "            features['channel'] = ch_label\n",
    "            features['unc_delta'] = unc_bandpowers[0]\n",
    "            features['unc_theta'] = unc_bandpowers[1]\n",
    "            features['unc_alpha'] = unc_bandpowers[2]\n",
    "            features['unc_beta'] = unc_bandpowers[3]\n",
    "            features['fp_slow_theta'] = freq_prev_slow_theta\n",
    "            features['fp_prealpha'] = freq_prev_prealpha\n",
    "            features['fp_alpha'] = freq_prev_alpha\n",
    "            features['dom_freq'] = dom_freq\n",
    "            features['dom_freq_var'] = dom_freq_var\n",
    "            features['beta_cf'] = betas[0]\n",
    "            features['beta_pw'] = betas[1]\n",
    "            features['beta_bw'] = betas[2]\n",
    "            features['extalphas_cf'] = extalphas[0]\n",
    "            features['extalphas_pw'] = extalphas[1]\n",
    "            features['extalphas_bw'] = extalphas[2]\n",
    "            features['exponent'] = aperiodic[1]\n",
    "            features['offset'] = aperiodic[0]\n",
    "            features['r_squared'] = fit[0]\n",
    "            features['error'] = fit[1]\n",
    "            # Median score for the best 20 epochs\n",
    "            median_scorEpochs = np.median(scores[best_20_indices])\n",
    "            # Scores of the best 20 epochs \n",
    "            all_scorEpochs = list(scores[best_20_indices])\n",
    "            features['median_scorEpochs'] = median_scorEpochs\n",
    "            features['all_scorEpochs'] = all_scorEpochs\n",
    "            dict_list.append(features)\n",
    "df = pd.DataFrame(dict_list)\n",
    "\n",
    "df.to_feather('D:/data_analysis/papers_alberto/cau_combat/features_data/fooof_korea.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "dict_list = []\n",
    "for eeg_file in eegs: #read and preload dataset\n",
    "    subject_info = layout.parse_file_entities(eeg_file)\n",
    "    session = subject_info.get('session')\n",
    "    subject = subject_info.get('subject')\n",
    "    bids_path = BIDSPath(subject=subject, session=session, task=task, root=bids_root, datatype='eeg')\n",
    "    epochs = mne.read_epochs(eeg_file, preload = True)\n",
    "    mapping = {'T3': 'T7', 'T4': 'T8', 'T5': 'P7', 'T6': 'P8'}\n",
    "    epochs.rename_channels(mapping)\n",
    "    standardize(epochs) #standardize ch_names    \n",
    "    keep = ['Fp1','Fp2', 'F3','F4', 'Fz', 'F7', 'F8',  'C3', 'C4', 'Cz' , 'T7','T8',  'P7' ,'P8', 'P3','P4', 'Pz', 'O1', 'O2']\n",
    "    epochs.pick_channels(keep)\n",
    "    epochs = epochs.filter(l_freq=1, h_freq=45) #bandpassing 1-30Hz\n",
    "    downsample = 128 #downsampling to 128Hz\n",
    "    epochs.resample(downsample)\n",
    "    sf = downsample\n",
    "    nepochs,nchannels,npoints = epochs._data.shape\n",
    "    if nepochs >= 20:\n",
    "        clean_epochs = epochs\n",
    "         # get shape of clean epoched data and reshape to fit scorEpochs input requirements\n",
    "        continuous_signal = np.reshape(clean_epochs,(nchannels,nepochs*npoints),order='F') # final shape nchannels, npoints\n",
    "        t_ep = 5 #epoch length\n",
    "        fs = downsample #sampling freq\n",
    "        freq = [1, 30] #interest freq\n",
    "        cfg = {'freqRange':freq, 'fs':fs, 'windowL':t_ep}\n",
    "        idx_best, epoch, scores = scorEpochs(cfg, continuous_signal)\n",
    "        # Get the top 20 epoch indices from idx_best\n",
    "        best_20_indices = idx_best[:20]\n",
    "        # Use these indices to extract the best 20 epochs from clean_epochs\n",
    "        best_20_epochs = epochs[best_20_indices]\n",
    "        # Optionally, you can concatenate the epochs (though it's not strictly necessary)\n",
    "        epochs_c = mne.concatenate_epochs([best_20_epochs], add_offset=False, on_mismatch='raise', verbose=None)\n",
    "\n",
    "        epochs = epochs_c\n",
    "        epochs.set_montage(\"standard_1005\")\n",
    "        nepochs,nchannels,npoints = epochs._data.shape\n",
    "        correct_channels = keep\n",
    "        epochs.reorder_channels(correct_channels)\n",
    "        channels = epochs.info['ch_names']\n",
    "        for ch,ch_label in enumerate(channels):\n",
    "            psds, freqs = mne.time_frequency.psd_array_multitaper(epochs.get_data()[:,ch,:], sfreq = sf,fmin=1, fmax=30, n_jobs=-1)\n",
    "            psds = np.median(psds, 0) # get median psd vector per channel\n",
    "            fm = FOOOF(peak_width_limits=[1, 8], min_peak_height=0.05, max_n_peaks=6)\n",
    "            # Initialize FOOOF object\n",
    "            # Define frequency range across which to model the spectrum\n",
    "            freq_range = [1, 30]\n",
    "            fm.fit(freqs, psds, freq_range)   # Fit the power spectrum model\n",
    "            osc_fit = fm._peak_fit\n",
    "            psd_fit = fm.fooofed_spectrum_\n",
    "            ap_fit = fm._ap_fit\n",
    "            spect_flat = fm._spectrum_flat\n",
    "            aperiodic = fm._spectrum_peak_rm\n",
    "            for freq in freqs:\n",
    "                features = {}\n",
    "                features['center'] = 'korea'\n",
    "                features['subject'] = ('kor_' + subject)\n",
    "                features['channel'] = ch_label\n",
    "                features['frequency'] = freq\n",
    "                features['psd'] = np.log10(psds[freqs==freq][0])\n",
    "                features['psd_fit'] = psd_fit[freqs==freq][0]\n",
    "                features['osc'] = spect_flat[freqs==freq][0]            \n",
    "                features['osc_fit'] = osc_fit[freqs==freq][0]\n",
    "                features['ap'] = aperiodic[freqs==freq][0]\n",
    "                features['ap_fit'] = ap_fit[freqs==freq][0]\n",
    "                dict_list.append(features)\n",
    "df = pd.DataFrame(dict_list)\n",
    "df.to_feather('D:/data_analysis/papers_alberto/cau_combat/features_data/spectrum_fooof_korea.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_channel_power(df):\n",
    "    unique_participants = df['subject'].unique()\n",
    "    unique_channels = df['channel'].unique()\n",
    "    colors = plt.cm.get_cmap('tab10')(np.linspace(0, 1, len(unique_participants)))\n",
    "\n",
    "    fig, axs = plt.subplots(figsize=(15, 8), ncols=3)\n",
    "\n",
    "    for idx, ax in enumerate(axs):\n",
    "        # Get columns to plot\n",
    "        column = ['psd_fit', 'ap_fit', 'osc_fit'][idx]\n",
    "        ax.set_title(column.capitalize() + ' Power')\n",
    "        ax.set_xlabel('Frequency (Hz)')\n",
    "        if idx == 0:\n",
    "            ax.set_ylabel('Power (log10)')\n",
    "        else:\n",
    "            ax.set_ylabel('Power (a.u)')\n",
    "\n",
    "        # Get channel mean for each participant\n",
    "        for i, participant in enumerate(unique_participants):\n",
    "            participant_df = df[df['subject'] == participant]\n",
    "            participant_avg = participant_df.groupby(['frequency'])[column].mean()\n",
    "            ax.plot(participant_avg.index, participant_avg.values, color=colors[i], alpha=0.2, label=None)\n",
    "\n",
    "        ax.legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example\n",
    "plot_channel_power(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge features & meta-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Dataset participants.tsv metadata and merge demographic variables with features \n",
    "\n",
    "metadata_cols = ['participant_id', 'ad_syndrome' , 'age'] # names of the metadata columns to be merged\n",
    "\n",
    "participants_file = 'D:/data_analysis/eeg_datasets/bids/korea/participants.tsv'\n",
    "participants_df = pd.read_csv(participants_file, sep=\"\\t\")\n",
    "participants_df.columns= participants_df.columns.str.lower() # To set all the column names in lower case\n",
    "participants_df['participant_id'] = participants_df['participant_id'].str.replace('sub-', 'kor_')  # Drop the \"sub-\" prefix used for BIDS standardization\n",
    "participants_df = participants_df.loc[:, metadata_cols]  # Filter the DataFrame to include only the specified columns\n",
    "\n",
    "entropies_file = 'D:/data_analysis/papers_alberto/cau_combat/features_data/ent_korea.feather'\n",
    "entropies_df = pd.read_feather(entropies_file)\n",
    "entropies_df.rename(columns = {'subject':'participant_id'}, inplace = True)\n",
    "\n",
    "\n",
    "bandpowers_file = 'D:/data_analysis/papers_alberto/cau_combat/features_data/fooof_korea.feather'\n",
    "bandpowers_df = pd.read_feather(bandpowers_file)\n",
    "bandpowers_df.rename(columns = {'subject':'participant_id'}, inplace = True)\n",
    "\n",
    "# List of channels to filter\n",
    "channels = ['Fp1', 'Fp2', 'F3', 'F4', 'Fz', 'F7', 'F8', 'C3', 'C4', 'Cz', 'T7', 'T8', 'P7', 'P8', 'P3', 'P4', 'Pz', 'O1', 'O2']\n",
    "\n",
    "# Filter the DataFrame\n",
    "bandpowers = bandpowers_df[bandpowers_df['channel'].isin(channels)]\n",
    "\n",
    "\n",
    "korea_df = pd.DataFrame(pd.merge(participants_df, entropies_df, on=[\"participant_id\"])) \n",
    "korea_df = pd.merge(korea_df, bandpowers, on=[\"participant_id\", \"center\", \"channel\"])\n",
    "\n",
    "# Full CAU dataset (n = 1300)\n",
    "\n",
    "korea_df.to_csv('D:/data_analysis/papers_alberto/cau_combat/features_data/korea_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiffusionEEG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
